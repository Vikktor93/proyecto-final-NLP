{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7395a0",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://i.ibb.co/v3CvVz9/udd-short.png\" width=\"150\"/>\n",
    "    <br>\n",
    "    <strong>Universidad del Desarrollo</strong><br>\n",
    "    <em>Magíster en Data Science</em><br>\n",
    "    <em>Profesora: Maria Paz Raveau</em><br>\n",
    "    <em>Asignatura: Procesamiento de Lenguaje Natural</em><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d451bd4",
   "metadata": {},
   "source": [
    "## **Proyecto Final PLN: Sesgo en Word Embeddings (WEAT)**\n",
    "**Curso:** Procesamiento de Lenguaje Natural – Magíster en Data Science UDD (2025) \n",
    "\n",
    "*Fecha de Entrega: Martes 02, Septiembre 2025.*\n",
    "\n",
    "**Estudiantes**: Victor Saldivia Vera, Cristian Tobar, Joaquin Leiva.\n",
    "\n",
    "### **1. Instrucciones y Enunciado**\n",
    "\n",
    "Se ha dicho que los modelos de WordEmbeddings contienen sesgos. Se solicita investigar qué\n",
    "sesgos han sido identificados en la literatura, y buscar al menos una métrica para evaluar alguno de\n",
    "estos sesgos. Testear esta métrica en algunos de los modelos pre-entrenados en idioma\n",
    "español y concluir respecto al sesgo encontrado (o no encontrado). \n",
    "\n",
    "El entregable es un reporte de las siguientes características:\n",
    "\n",
    "- **Introducción** sobre sesgos que han sido identificados en modelos de embeddings *(máximo\n",
    "1200 palabras, 10 puntos)*. Se evaluará el uso de literatura pertinente. Ser informativo:\n",
    "¿Qué sesgo se han identificados?, ¿Qué corpus fue utilizado?, ¿Con qué métricas? En esta sección deberá\n",
    "explicitar que sesgo se eligió para testear, qué métrica y con qué modelo pre-entrenado en\n",
    "español se trabajó. Todo debe estar debidamente referenciado.\n",
    "- **Método:** explicar qué métrica fue utilizada para evaluar la presencia del sesgo. Se debe explicar la\n",
    "métrica, no el código. El código se entregará como anexo y no será necesariamente\n",
    "revisado *(máximo 500 palabras, 5 puntos)*.\n",
    "- **Resultados/Conclusiones:** Reportar los resultados y concluir respecto a la ideonidad\n",
    "de la métrica usada, y a la presencia o no de sesgo en el corpus. ¿Son consistentes sus\n",
    "resultados con los reportados en la literatura? *(700 palabras máximo)*.\n",
    "- Referencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562471c1",
   "metadata": {},
   "source": [
    "### **2. Breve Introducción**\n",
    "En este cuaderno Jupyter se implementa el **Word Embedding Association Test (WEAT)** para evaluar la presencia de sesgos de género en *word embeddings* en español.  \n",
    "\n",
    "Se utiliza como **modelo principal** `fastText` en español (`cc.es.300.vec`) y opcionalmente `SBW` (Spanish Billion Words, word2vec).  \n",
    "\n",
    "El enfoque está en el **sesgo de género** (hombre ↔ carrera vs mujer ↔ familia)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823a0a2",
   "metadata": {},
   "source": [
    "### **3. Imports y Seeds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07c5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\nlp_weat\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "c:\\Users\\victo\\anaconda3\\envs\\nlp_weat\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os, gzip, zipfile, shutil, json, random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80e01e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b07f74a",
   "metadata": {},
   "source": [
    "### **3. Ajustes de Rutas del Proyecto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c485da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raíz del proyecto: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\n",
      "Embeddings directorio: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\n",
      "Outputs directorio: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Se sube un nivel para llegar a la raíz del proyecto:\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "# Carpeta de embeddings y de salidas en la raíz del repositorio\n",
    "DATA_DIR   = PROJECT_ROOT / \"data_embeddings\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "# Se crean las carpetas si no existen\n",
    "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Raíz del proyecto:\", PROJECT_ROOT)\n",
    "print(\"Embeddings directorio:\", DATA_DIR)\n",
    "print(\"Outputs directorio:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d744c249",
   "metadata": {},
   "source": [
    "### **4.  Modelos Pre-entrenados en Español**\n",
    "\n",
    "Para este proyecto utilizamos embeddings **pre-entrenados**, que son vectores generados a partir de grandes corpus de texto.  \n",
    "\n",
    "- **fastText (cc.es.300.vec.gz)** → entrenado en *Common Crawl + Wikipedia*, con 300 dimensiones y subpalabras.  \n",
    "- **SBW word2vec (sbw-300-min5.vec.gz)** → entrenado en *Spanish Billion Words Corpus* (aprox 1.5 Billones de tokens).  \n",
    "\n",
    "*Observaciones:*\n",
    "\n",
    "- *Ambos se deben descargar y colocar en la carpeta `data_embeddings/`*\n",
    "- *El archivo `.gz` se descomprime a `.vec` antes de cargarse con `gensim`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2fdc0",
   "metadata": {},
   "source": [
    "Se crea un `función` de nombre `gunzip` que descomprime un `.gz` a `.vec` (texto). No hace nada si ya existe el `.vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46df9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descomprimiendo .gz → cc.es.300.vec\n",
      "Listo: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\cc.es.300.vec\n",
      "Descomprimiendo .zip → C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\n",
      "Listo: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\SBW-vectors-300-min5.txt\n",
      "\n",
      "Resumen:\n",
      " - fastText .vec: True C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\cc.es.300.vec\n",
      " - SBW (Kaggle .txt): True C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\SBW-vectors-300-min5.txt\n",
      " - SBW (legado .vec): False C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\sbw-300-min5.vec\n"
     ]
    }
   ],
   "source": [
    "# FastText: .gz -> .vec\n",
    "FASTTEXT_VEC_GZ  = DATA_DIR / \"cc.es.300.vec.gz\"\n",
    "FASTTEXT_VEC_TXT = DATA_DIR / \"cc.es.300.vec\"\n",
    "\n",
    "# SBW: esta en formato .zip\n",
    "# Se busca cualquier zip que empiece con \"SBW-vectors\"\n",
    "SBW_ZIP = None\n",
    "for f in DATA_DIR.glob(\"SBW-vectors*.zip\"):\n",
    "    SBW_ZIP = f\n",
    "    break\n",
    "\n",
    "# Archivo esperado después de descomprimir\n",
    "SBW_VEC_TXT = DATA_DIR / \"SBW-vectors-300-min5.txt\"\n",
    "\n",
    "# (Compatibilidad) SBW legado: .gz -> .vec\n",
    "SBW_VEC_GZ_LEGACY  = DATA_DIR / \"sbw-300-min5.vec.gz\"\n",
    "SBW_VEC_TXT_LEGACY = DATA_DIR / \"sbw-300-min5.vec\"\n",
    "\n",
    "def gunzip(src_gz: Path, dst_txt: Path):\n",
    "    if dst_txt.exists():\n",
    "        print(\"Ya existe:\", dst_txt)\n",
    "        return True\n",
    "    if not src_gz.exists():\n",
    "        print(\"No se encuentra:\", src_gz)\n",
    "        return False\n",
    "    print(\"Descomprimiendo .gz →\", dst_txt.name)\n",
    "    with gzip.open(src_gz, 'rb') as f_in, open(dst_txt, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "    print(\"Listo:\", dst_txt)\n",
    "    return True\n",
    "\n",
    "def unzip(src_zip: Path, expected_txt: Path, dest_dir: Path):\n",
    "    if expected_txt.exists():\n",
    "        print(\"Ya existe:\", expected_txt)\n",
    "        return True\n",
    "    if not src_zip or not src_zip.exists():\n",
    "        print(\"No se encontró el archivo zip:\", src_zip)\n",
    "        return False\n",
    "    print(\"Descomprimiendo .zip →\", dest_dir)\n",
    "    with zipfile.ZipFile(src_zip, 'r') as zf:\n",
    "        zf.extractall(dest_dir)\n",
    "        names = zf.namelist()\n",
    "    if expected_txt.exists():\n",
    "        print(\"Listo:\", expected_txt)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Extraído, pero no se encontro el archivo esperado:\", expected_txt.name)\n",
    "        print(\"Archivos en el zip:\", names[:10], \"... (máximo 10)\")\n",
    "        return False\n",
    "\n",
    "# PROCESO\n",
    "# 1. fastText (.gz → .vec)\n",
    "_ = gunzip(FASTTEXT_VEC_GZ, FASTTEXT_VEC_TXT)\n",
    "\n",
    "# 2. SBW Kaggle (.zip/.txt.zip → .txt)\n",
    "ok_sbw = False\n",
    "if SBW_ZIP:\n",
    "    ok_sbw = unzip(SBW_ZIP, SBW_VEC_TXT, DATA_DIR)\n",
    "\n",
    "# 3. Si no hubo zip Kaggle, intentar legado (.gz → .vec)\n",
    "if not ok_sbw and SBW_VEC_GZ_LEGACY.exists():\n",
    "    ok_sbw = gunzip(SBW_VEC_GZ_LEGACY, SBW_VEC_TXT_LEGACY)\n",
    "\n",
    "print(\"\\nResumen:\")\n",
    "print(\" - fastText .vec:\", FASTTEXT_VEC_TXT.exists(), FASTTEXT_VEC_TXT)\n",
    "print(\" - SBW (Kaggle .txt):\", SBW_VEC_TXT.exists(), SBW_VEC_TXT)\n",
    "print(\" - SBW (legado .vec):\", SBW_VEC_TXT_LEGACY.exists(), SBW_VEC_TXT_LEGACY)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bae1a",
   "metadata": {},
   "source": [
    "### **5. Carga de Emebeddings**\n",
    "\n",
    "Se cargan los **.vec/.txt** con `gensim` (formato Word2Vec/fastText de **texto**).\n",
    "- En este caso **NO** se necesita instalar la librería `fasttext` nativa.\n",
    "- Se puede utilizar `limit=N` para pruebas rápidas (carga parcial del vocabulario).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1a953dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\cc.es.300.vec  (binary=False, limit=None)\n",
      "Vocabulario cargado: 2,000,000 tokens | Dimensiones: 300\n",
      "Cargando: C:\\Users\\victo\\OneDrive\\Documentos\\Programming-2025\\UDD-2025\\NLP\\Proyecto\\data_embeddings\\SBW-vectors-300-min5.txt  (binary=False, limit=None)\n",
      "Vocabulario cargado: 1,000,653 tokens | Dimensiones: 300\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path: Path, binary: bool = False, limit: int = None) -> KeyedVectors:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No existe el archivo: {path}\")\n",
    "    print(f\"Cargando: {path}  (binary={binary}, limit={limit})\")\n",
    "    kv = KeyedVectors.load_word2vec_format(str(path), binary=binary, limit=limit, unicode_errors=\"ignore\")\n",
    "    print(f\"Vocabulario cargado: {len(kv.index_to_key):,} tokens | Dimensiones: {kv.vector_size}\")\n",
    "    return kv\n",
    "\n",
    "fasttext_kv = None\n",
    "sbw_kv = None\n",
    "\n",
    "# FastText principal\n",
    "if FASTTEXT_VEC_TXT.exists():\n",
    "    fasttext_kv = load_embeddings(FASTTEXT_VEC_TXT, binary=False, limit=None)\n",
    "else:\n",
    "    print(\"Sugerencia: coloca 'cc.es.300.vec.gz' en ../data_embeddings/ y ejecuta la celda de descompresión\")\n",
    "\n",
    "# SBW preferente (Kaggle .txt)\n",
    "if SBW_VEC_TXT.exists():\n",
    "    sbw_kv = load_embeddings(SBW_VEC_TXT, binary=False, limit=None)\n",
    "# Alternativa (legado .vec)\n",
    "elif SBW_VEC_TXT_LEGACY.exists():\n",
    "    sbw_kv = load_embeddings(SBW_VEC_TXT_LEGACY, binary=False, limit=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19043132",
   "metadata": {},
   "source": [
    "### **6. Word Embedding Association Test (WEAT)**\n",
    "\n",
    "Para cada palabra $w$:\n",
    "\n",
    "$$\n",
    "s(w,A,B) = \\frac{1}{|A|}\\sum_{a\\in A}\\cos(\\vec w,\\vec a) - \\frac{1}{|B|}\\sum_{b\\in B}\\cos(\\vec w,\\vec b)\n",
    "$$\n",
    "\n",
    "El **tamaño de efecto** (d de Cohen):\n",
    "\n",
    "$$\n",
    "d = \\frac{\\mu_{x \\in X} \\, s(x,A,B) - \\mu_{y \\in Y} \\, s(y,A,B)}\n",
    "         {\\sigma_{w \\in X \\cup Y} \\, s(w,A,B)}\n",
    "$$\n",
    "\n",
    "La **significancia** se estima con **permutaciones** de $X \\cup Y$ (p-valor uni/bilateral).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4d824d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcosine\u001b[39m(u: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray, v: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m      2\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(u) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(v))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m denom \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def cosine(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    if denom == 0.0:\n",
    "        return 0.0\n",
    "    return float(np.dot(u, v) / denom)\n",
    "\n",
    "def mean_cosine(w_vec: np.ndarray, attr_vecs: List[np.ndarray]) -> float:\n",
    "    if not attr_vecs:\n",
    "        return 0.0\n",
    "    return float(np.mean([cosine(w_vec, a) for a in attr_vecs]))\n",
    "\n",
    "def s_w_A_B(w: str, kv: KeyedVectors, A: List[str], B: List[str]) -> float:\n",
    "    return mean_cosine(kv[w], [kv[a] for a in A]) - mean_cosine(kv[w], [kv[b] for b in B])\n",
    "\n",
    "def weat_effect_size(X: List[str], Y: List[str], A: List[str], B: List[str], kv: KeyedVectors) -> float:\n",
    "    s_X = [s_w_A_B(x, kv, A, B) for x in X]\n",
    "    s_Y = [s_w_A_B(y, kv, A, B) for y in Y]\n",
    "    mu_X, mu_Y = np.mean(s_X), np.mean(s_Y)\n",
    "    s_all = np.array(s_X + s_Y, dtype=float)\n",
    "    sigma = np.std(s_all, ddof=1) if len(s_all) > 1 else 0.0\n",
    "    return float((mu_X - mu_Y) / sigma) if sigma > 0 else float('nan')\n",
    "\n",
    "def weat_permutation_test(X: List[str], Y: List[str], A: List[str], B: List[str],\n",
    "                          kv: KeyedVectors, n_samples: int = 10000, seed: int = 42) -> Dict[str, float]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    obs = weat_effect_size(X, Y, A, B, kv)\n",
    "    pool = np.array(X + Y)\n",
    "    nX = len(X)\n",
    "\n",
    "    null_effects = np.empty(n_samples, dtype=float)\n",
    "    for i in range(n_samples):\n",
    "        perm = rng.permutation(pool)\n",
    "        Xp, Yp = list(perm[:nX]), list(perm[nX:])\n",
    "        null_effects[i] = weat_effect_size(Xp, Yp, A, B, kv)\n",
    "\n",
    "    if np.isnan(obs):\n",
    "        p_one = np.nan; p_two = np.nan\n",
    "    else:\n",
    "        p_one = float(np.mean(null_effects >= obs)) if obs >= 0 else float(np.mean(null_effects <= obs))\n",
    "        p_two = float(np.mean(np.abs(null_effects) >= abs(obs)))\n",
    "\n",
    "    return {\n",
    "        \"effect_observed\": float(obs),\n",
    "        \"null_mean\": float(np.nanmean(null_effects)),\n",
    "        \"null_std\": float(np.nanstd(null_effects, ddof=1)),\n",
    "        \"p_value_one_sided\": p_one,\n",
    "        \"p_value_two_sided\": p_two,\n",
    "        \"n_permutations\": int(n_samples)\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_weat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
